{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### White-box Evasion attacks (40m)\n",
    "In this part of the assignment, you will be training your own model for an image classification task and implementing white-box adversarial attacks to confuse the model. Please refer to resources 1, 2, and 3 for more information and code examples.\n",
    "\n",
    "\n",
    "Tasks\n",
    "1. Download the MNIST dataset, and train a deep learning model of your own from scratch for this dataset. You must write a report on your classification accuracy for the train and test data. (5m)\n",
    "2. Pick an image x from the dataset. Create xadv using the methods shown listed below. Visualize and compare the images x, xadv , and the adversarial noise\n",
    "Œ¥ = xadv ‚àí x in your report (refer to the image above). Also, report the Losses L(x, yoriginal‚àílabel); Œ∏) (where Œ∏ refers to the model parameters),\n",
    "L(xadv,yoriginal‚àílabel);Œ∏)and L(xadv,ychanged‚àílabel);Œ∏)along with the predicted label achieved after the attack. Assume the default attack budget Œµ\n",
    "to be 0.2, feel free to play around with more values of Œµ. (15m) \n",
    "\n",
    "    a) Fast Gradient Sign Method (FGSM)\n",
    "\n",
    "        i. Untargeted \n",
    "\n",
    "        ii. Targeted\n",
    "    b) Projected Gradient Descent (PDG) \n",
    "\n",
    "        i. Untargeted\n",
    "    \n",
    "        ii. Targeted\n",
    "\n",
    "    \n",
    "3. Apply all the above attacks on the test set using 3 or more different budget-ùúñ values (any 3 values from 0.05 to 0.3). Report the test accuracies for these 3 cases in all the 4 scenarios. (10m)\n",
    "\n",
    "\n",
    "<img src=\"/Users/hrithik/Library/CloudStorage/OneDrive-InternationalInstituteofInformationTechnology/S24/RSAI/S24-RSAI-Asgn-1-Poisioning-Attack/Task 2/Screenshot 2024-02-17 at 19.57.59.png\" alt=\"Table\">\n",
    "\n",
    "1. Answer the following questions. support your answers with empirical evidence and plots. (10m)\n",
    "\n",
    "a) Plot and analyze the accuracy change with respect to each of the epsilon values. How do the images change for each of these values? Will a higher epsilon value mean a good attack? If not, why?\n",
    "\n",
    "b) Which attack is the best? List down the pros and cons of each attack. When would you use a targeted attack vs an untargeted attack?\n",
    "\n",
    "5. (BONUS) Perform Adversarial training on your model and report the new accuracies (20m)\n",
    "\n",
    "Resources:\n",
    "1. https://adversarial-ml-tutorial.org/introduction/\n",
    "2. https://pytorch.org/tutorials/beginner/fgsm_tutorial.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/venv1/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9912422/9912422 [00:01<00:00, 6311450.00it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28881/28881 [00:00<00:00, 31702615.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1648877/1648877 [00:01<00:00, 1218891.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4542/4542 [00:00<00:00, 6912383.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Finished Training\n",
      "Accuracy of the network on the 10000 test images: 98 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Download and load the MNIST dataset\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define a simple CNN model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 5 * 5, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:  # print every 1000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 1000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Evaluate the model on test data\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Loss: 0.00015090758097358048\n",
      "FGSM Loss: 2.14235258102417\n",
      "PGD Loss: 0.20758511126041412\n",
      "Original Labels: tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7])\n",
      "FGSM Predicted Labels: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])\n",
      "FGSM Predicted Outputs: tensor([[-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721],\n",
      "        [-0.4319, -2.8434,  0.3864,  2.3953, -2.5583, -2.3451, -3.6755,  1.4188,\n",
      "          2.8630, -1.5721]], grad_fn=<AddmmBackward0>)\n",
      "PGD Predicted Label: tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7])\n",
      "FGSM Predicted Outputs Shape: torch.Size([64, 10])\n",
      "FGSM Predicted Labels Shape: torch.Size([64])\n",
      "FGSM Predicted Labels:\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "# Implement Fast Gradient Sign Method (FGSM)\n",
    "def fgsm_attack(image, epsilon, data_grad):\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    perturbed_image = image + epsilon * sign_data_grad\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    return perturbed_image\n",
    "\n",
    "# Implement Projected Gradient Descent (PGD)\n",
    "def pgd_attack(image, epsilon, data_grad, alpha, num_steps):\n",
    "    perturbed_image = image.clone().detach()\n",
    "    for _ in range(num_steps):\n",
    "        perturbed_image.requires_grad = True\n",
    "        output = net(perturbed_image)\n",
    "        loss = criterion(output, true_label)\n",
    "        net.zero_grad()\n",
    "        loss.backward()\n",
    "        data_grad = perturbed_image.grad.data\n",
    "        perturbed_image = perturbed_image + alpha * torch.sign(data_grad)\n",
    "        perturbed_image = torch.max(torch.min(perturbed_image, image + epsilon), image - epsilon).detach()\n",
    "    return perturbed_image\n",
    "\n",
    "# Choose an image from the dataset\n",
    "# Choose an image from the dataset\n",
    "# image, true_label = testset[0][0].unsqueeze(0), torch.tensor([testset[0][1]])  # Convert true_label to tensor\n",
    "image, true_label = testset[0][0].unsqueeze(0).repeat(64, 1, 1, 1), torch.tensor([testset[0][1]] * 64)  # Convert true_label to tensor\n",
    "\n",
    "# FGSM Attack\n",
    "epsilon = 0.2\n",
    "image.requires_grad = True\n",
    "output = net(image)\n",
    "loss = criterion(output, true_label)  # Ensure true_label is a tensor\n",
    "net.zero_grad()\n",
    "loss.backward()\n",
    "data_grad = image.grad.data\n",
    "perturbed_image_fgsm = fgsm_attack(image, epsilon, data_grad)\n",
    "\n",
    "# PGD Attack\n",
    "alpha = 0.01\n",
    "num_steps = 40\n",
    "perturbed_image_pgd = pgd_attack(image, epsilon, data_grad, alpha, num_steps)\n",
    "\n",
    "# Visualize and compare images\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# fig, axs = plt.subplots(1, 3)\n",
    "# axs[0].imshow(image.squeeze().detach().numpy(), cmap='gray')\n",
    "# axs[0].set_title('Original')\n",
    "\n",
    "# axs[1].imshow(perturbed_image_fgsm.squeeze().detach().numpy(), cmap='gray')\n",
    "# axs[1].set_title('FGSM')\n",
    "\n",
    "# axs[2].imshow(perturbed_image_pgd.squeeze().detach().numpy(), cmap='gray')\n",
    "# axs[2].set_title('PGD')\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "fig, axs = plt.subplots(1, 3)\n",
    "axs[0].imshow(image[0].squeeze().detach().numpy(), cmap='gray')  # Select the first image from the batch\n",
    "axs[0].set_title('Original')\n",
    "\n",
    "axs[1].imshow(perturbed_image_fgsm[0].squeeze().detach().numpy(), cmap='gray')  # Select the first perturbed image from the batch\n",
    "axs[1].set_title('FGSM')\n",
    "\n",
    "axs[2].imshow(perturbed_image_pgd[0].squeeze().detach().numpy(), cmap='gray')  # Select the first perturbed image from the batch\n",
    "axs[2].set_title('PGD')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Evaluate the model on perturbed images\n",
    "output_fgsm = net(perturbed_image_fgsm)\n",
    "output_pgd = net(perturbed_image_pgd)\n",
    "_, predicted_fgsm = torch.max(output_fgsm, 1)\n",
    "_, predicted_pgd = torch.max(output_pgd, 1)\n",
    "\n",
    "loss_original = criterion(output, true_label)\n",
    "loss_fgsm = criterion(output_fgsm, true_label)\n",
    "loss_pgd = criterion(output_pgd, true_label)\n",
    "\n",
    "print(\"Original Loss:\", loss_original.item())\n",
    "print(\"FGSM Loss:\", loss_fgsm.item())\n",
    "print(\"PGD Loss:\", loss_pgd.item())\n",
    "\n",
    "# print(\"Original Label:\", true_label.item())\n",
    "print(\"Original Labels:\", true_label)\n",
    "# print(\"FGSM Predicted Label:\", predicted_fgsm.item())\n",
    "print(\"FGSM Predicted Labels:\", predicted_fgsm)\n",
    "print(\"FGSM Predicted Outputs:\", output_fgsm)\n",
    "\n",
    "\n",
    "# print(\"PGD Predicted Label:\", predicted_pgd.item())\n",
    "print(\"PGD Predicted Label:\", predicted_pgd)\n",
    "\n",
    "\n",
    "epsilons = [0.05, 0.15, 0.25]  # different values of epsilon\n",
    "print(\"FGSM Predicted Outputs Shape:\", output_fgsm.shape)\n",
    "print(\"FGSM Predicted Labels Shape:\", predicted_fgsm.shape)\n",
    "print(\"FGSM Predicted Labels:\")\n",
    "for label in predicted_fgsm:\n",
    "    print(label.item())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FGSM Untargeted:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (64) to match target batch_size (1).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/hrithik/Library/CloudStorage/OneDrive-InternationalInstituteofInformationTechnology/S24/RSAI/S24-RSAI-Asgn-1-Poisioning-Attack/Task 2/A1T2_2023900021_WhiteBoxAttack.ipynb Cell 4\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hrithik/Library/CloudStorage/OneDrive-InternationalInstituteofInformationTechnology/S24/RSAI/S24-RSAI-Asgn-1-Poisioning-Attack/Task%202/A1T2_2023900021_WhiteBoxAttack.ipynb#X10sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFGSM Untargeted:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hrithik/Library/CloudStorage/OneDrive-InternationalInstituteofInformationTechnology/S24/RSAI/S24-RSAI-Asgn-1-Poisioning-Attack/Task%202/A1T2_2023900021_WhiteBoxAttack.ipynb#X10sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39mfor\u001b[39;00m epsilon \u001b[39min\u001b[39;00m epsilons:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/hrithik/Library/CloudStorage/OneDrive-InternationalInstituteofInformationTechnology/S24/RSAI/S24-RSAI-Asgn-1-Poisioning-Attack/Task%202/A1T2_2023900021_WhiteBoxAttack.ipynb#X10sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     accuracy \u001b[39m=\u001b[39m test_attack(fgsm_attack, \u001b[39m\"\u001b[39;49m\u001b[39mFGSM\u001b[39;49m\u001b[39m\"\u001b[39;49m, testloader, epsilon)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hrithik/Library/CloudStorage/OneDrive-InternationalInstituteofInformationTechnology/S24/RSAI/S24-RSAI-Asgn-1-Poisioning-Attack/Task%202/A1T2_2023900021_WhiteBoxAttack.ipynb#X10sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEpsilon:\u001b[39m\u001b[39m\"\u001b[39m, epsilon, \u001b[39m\"\u001b[39m\u001b[39mAccuracy:\u001b[39m\u001b[39m\"\u001b[39m, accuracy)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hrithik/Library/CloudStorage/OneDrive-InternationalInstituteofInformationTechnology/S24/RSAI/S24-RSAI-Asgn-1-Poisioning-Attack/Task%202/A1T2_2023900021_WhiteBoxAttack.ipynb#X10sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39m# PGD Untargeted\u001b[39;00m\n",
      "\u001b[1;32m/Users/hrithik/Library/CloudStorage/OneDrive-InternationalInstituteofInformationTechnology/S24/RSAI/S24-RSAI-Asgn-1-Poisioning-Attack/Task 2/A1T2_2023900021_WhiteBoxAttack.ipynb Cell 4\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hrithik/Library/CloudStorage/OneDrive-InternationalInstituteofInformationTechnology/S24/RSAI/S24-RSAI-Asgn-1-Poisioning-Attack/Task%202/A1T2_2023900021_WhiteBoxAttack.ipynb#X10sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m num_classes \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hrithik/Library/CloudStorage/OneDrive-InternationalInstituteofInformationTechnology/S24/RSAI/S24-RSAI-Asgn-1-Poisioning-Attack/Task%202/A1T2_2023900021_WhiteBoxAttack.ipynb#X10sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m expanded_labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mrepeat(\u001b[39m1\u001b[39m, num_classes)  \u001b[39m# Repeat labels to match the number of classes\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/hrithik/Library/CloudStorage/OneDrive-InternationalInstituteofInformationTechnology/S24/RSAI/S24-RSAI-Asgn-1-Poisioning-Attack/Task%202/A1T2_2023900021_WhiteBoxAttack.ipynb#X10sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, expanded_labels)  \u001b[39m# Calculate loss using expanded labels\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hrithik/Library/CloudStorage/OneDrive-InternationalInstituteofInformationTechnology/S24/RSAI/S24-RSAI-Asgn-1-Poisioning-Attack/Task%202/A1T2_2023900021_WhiteBoxAttack.ipynb#X10sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m net\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hrithik/Library/CloudStorage/OneDrive-InternationalInstituteofInformationTechnology/S24/RSAI/S24-RSAI-Asgn-1-Poisioning-Attack/Task%202/A1T2_2023900021_WhiteBoxAttack.ipynb#X10sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/venv1/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/venv1/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/venv1/lib/python3.11/site-packages/torch/nn/modules/loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1179\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1180\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1181\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/venv1/lib/python3.11/site-packages/torch/nn/functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3051\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3052\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3053\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (64) to match target batch_size (1)."
     ]
    }
   ],
   "source": [
    "# def test_attack(attack_fn, attack_name, testloader, epsilon, alpha=None, num_steps=None):\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     for images, labels in testloader:\n",
    "#         images.requires_grad = True\n",
    "#         outputs = net(images)\n",
    "#         batch_size = images.size(0)\n",
    "#         expanded_labels = labels.repeat(outputs.size(1))  # Repeat labels to match the number of classes\n",
    "#         if expanded_labels.size(0) != outputs.size(0):\n",
    "#             expanded_labels = expanded_labels[:outputs.size(0)]\n",
    "#         loss = criterion(outputs, expanded_labels)  # Calculate loss using expanded labels\n",
    "#         net.zero_grad()\n",
    "#         loss.backward()\n",
    "#         data_grad = images.grad.data\n",
    "#         if attack_fn == fgsm_attack:\n",
    "#             perturbed_images = attack_fn(images, epsilon, data_grad)\n",
    "#         elif attack_fn == pgd_attack:\n",
    "#             perturbed_images = attack_fn(images, epsilon, data_grad, alpha, num_steps)\n",
    "#         perturbed_outputs = net(perturbed_images)\n",
    "#         _, predicted = torch.max(perturbed_outputs, 1)\n",
    "#         total += batch_size\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "#     accuracy = 100 * correct / total\n",
    "#     return accuracy\n",
    "\n",
    "\n",
    "def test_attack(attack_fn, attack_name, testloader, epsilon, alpha=None, num_steps=None):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in testloader:\n",
    "        images.requires_grad = True\n",
    "        outputs = net(images)\n",
    "        batch_size = images.size(0)\n",
    "        num_classes = outputs.size(1)\n",
    "        expanded_labels = labels.repeat(1, num_classes)  # Repeat labels to match the number of classes\n",
    "        loss = criterion(outputs, expanded_labels)  # Calculate loss using expanded labels\n",
    "        net.zero_grad()\n",
    "        loss.backward()\n",
    "        data_grad = images.grad.data\n",
    "        if attack_fn == fgsm_attack:\n",
    "            perturbed_images = attack_fn(images, epsilon, data_grad)\n",
    "        elif attack_fn == pgd_attack:\n",
    "            perturbed_images = attack_fn(images, epsilon, data_grad, alpha, num_steps)\n",
    "        \n",
    "        perturbed_outputs = net(perturbed_images)\n",
    "        perturbed_outputs = perturbed_outputs[:batch_size]  # Truncate to original batch size\n",
    "        expanded_labels = expanded_labels[:batch_size]  # Truncate labels to original batch size\n",
    "        loss = criterion(perturbed_outputs, expanded_labels)  # Recalculate loss with truncated outputs and labels\n",
    "\n",
    "        _, predicted = torch.max(perturbed_outputs, 1)\n",
    "        total += batch_size\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# FGSM Untargeted\n",
    "print(\"FGSM Untargeted:\")\n",
    "for epsilon in epsilons:\n",
    "    accuracy = test_attack(fgsm_attack, \"FGSM\", testloader, epsilon)\n",
    "    print(\"Epsilon:\", epsilon, \"Accuracy:\", accuracy)\n",
    "\n",
    "# PGD Untargeted\n",
    "print(\"\\nPGD Untargeted:\")\n",
    "for epsilon in epsilons:\n",
    "    accuracy = test_attack(pgd_attack, \"PGD\", testloader, epsilon, alpha=0.01, num_steps=40)\n",
    "    print(\"Epsilon:\", epsilon, \"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
